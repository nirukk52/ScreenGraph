#!/bin/bash
# Test Default Run Flow
# 
# Purpose: Automated end-to-end test of ScreenGraph run lifecycle using Chrome window
# 
# Flow:
# 1. Detect ports
# 2. Verify backend and frontend are running
# 3. Launch Chrome window and navigate to frontend
# 4. Click "Detect My First Drift" button
# 5. Wait for agent flow to complete (25-35 seconds)
# 6. Count screenshots discovered
# 7. Take proof screenshots
# 8. Report results with screenshot count
# 
# Prerequisites:
# - Backend running (encore run)
# - Frontend running (bun run dev)
# - Appium running (for device screenshots)
# - Cursor Chrome mode enabled
# 
# Usage:
#   @test-default-run in Cursor chat
#   or
#   .cursor/commands/test-default-run

# This is a command descriptor for Cursor AI
# The actual execution is handled by the cursor-chrome-window-mastery skill

cat << 'SKILL_INVOCATION'

# Test Default Run Flow - Command Execution

## Step 1: Configure Ports

**Action**: Load standard ports from `.env`

```bash
cd /Users/priyankalalge/ScreenGraph/Code/ScreenGraph
source .env 2>/dev/null || true
BACKEND_PORT=${BACKEND_PORT:-4000}
FRONTEND_PORT=${FRONTEND_PORT:-5173}
```

---

## Step 2: Verify Services Running

**Action**: Check if backend and frontend are accessible

```bash
# Check backend health
curl -sf http://localhost:${BACKEND_PORT}/health > /dev/null
if [ $? -ne 0 ]; then
  echo "❌ Backend not running on port ${BACKEND_PORT}"
  echo "Start with: cd backend && encore run"
  exit 1
fi

# Check frontend
curl -sf http://localhost:${FRONTEND_PORT} > /dev/null
if [ $? -ne 0 ]; then
  echo "❌ Frontend not running on port ${FRONTEND_PORT}"
  echo "Start with: cd frontend && bun run dev"
  exit 1
fi

echo "✅ Backend running on port ${BACKEND_PORT}"
echo "✅ Frontend running on port ${FRONTEND_PORT}"
```

---

## Step 3: Launch Chrome and Navigate

**Action**: Use cursor-chrome-window-mastery skill

**Invoke**:
```
Use the cursor-chrome-window-mastery skill to:
1. Launch Chrome window (Google Chrome mode)
2. Navigate to http://localhost:${FRONTEND_PORT}
3. Wait 2 seconds for page load
4. Take snapshot to get page structure
```

**Expected**:
- Page loads successfully
- Vite HMR connects
- No console errors
- "Detect My First Drift" button visible

---

## Step 4: Click "Detect My First Drift" Button

**Action**: Interact with landing page

**Invoke**:
```
Use the cursor-chrome-window-mastery skill to:
1. Get page snapshot
2. Find "Detect My First Drift" button ref
3. Click the button using the ref
4. Wait 2 seconds for navigation
```

**Expected**:
- Navigation to /run/{id} page
- POST /run request with status 200
- Timeline heading visible
- Run ID displayed

---

## Step 5: Wait for Agent Flow

**Action**: Wait for run to complete

**Invoke**:
```
Use the cursor-chrome-window-mastery skill to:
1. Wait 35 seconds for agent flow
   - EnsureDevice: 5s
   - ProvisionApp: 5s
   - Perceive: 20s
   - Processing: 5s
2. Monitor console logs during wait
3. Look for key events:
   - "agent.event.screenshot_captured"
   - "graph.screen.discovered"
   - "agent.run.finished"
```

**Expected**:
- WebSocket stream connects
- Console shows "[Graph Stream] Connected"
- Events appear in timeline
- Screenshot captured and processed

---

## Step 6: Count Screenshots

**Action**: Parse page to count discovered screenshots

**Invoke**:
```
Use the cursor-chrome-window-mastery skill to:
1. Take final snapshot of page
2. Parse accessibility tree for:
   - Section heading "Discovered Screens"
   - Child img elements with src="/artifacts/*"
3. Count img elements
4. Count graph nodes (if visible)
```

**Expected**:
- At least 1 screenshot discovered
- At least 1 graph node rendered
- Screenshots displayed with correct artifact URLs
- No broken images

**Parsing Pattern**:
```
Section "Discovered Screens"
  ├─ img src="/artifacts/bucket-id/screen-hash.png" [ref=...]
  ├─ img src="/artifacts/bucket-id/screen-hash.png" [ref=...]
  └─ img src="/artifacts/bucket-id/screen-hash.png" [ref=...]

Count: 3 screenshots
```

---

## Step 7: Take Proof Screenshots

**Action**: Capture visual evidence

**Invoke**:
```
Use the cursor-chrome-window-mastery skill to:
1. Take full page screenshot
2. Save to: .cursor/test-artifacts/test-default-run-{timestamp}.png
3. Capture both:
   - Timeline with events
   - Graph with nodes
   - Discovered screens section
```

**Expected**:
- Screenshot saved successfully
- All sections visible in screenshot
- Timestamp in filename for uniqueness

---

## Step 8: Report Results

**Action**: Generate test report

**Output Format**:
```markdown
# Test Default Run - Results

**Date**: {timestamp}
**Duration**: {seconds}s
**Ports**: Backend={BACKEND_PORT}, Frontend={FRONTEND_PORT}

## ✅ Test Status: PASSED / FAILED

### Run Information
- **Run ID**: 01XXXXXXXXXXXXXXXXXXXXX
- **Run Status**: completed / failed
- **Duration**: {run_duration}s

### Screenshots Discovered
- **Count**: {screenshot_count}
- **Display**: All screenshots rendered correctly
- **Artifacts**: All /artifacts/* requests succeeded

### Graph Visualization
- **Nodes Rendered**: {graph_node_count}
- **Layout**: Grid / Force layout
- **Interactive**: Yes / No

### Console Analysis
- **Errors**: {error_count}
- **Warnings**: {warning_count}
- **WebSocket**: Connected successfully / Failed

#### Key Console Logs:
\`\`\`
[Graph Stream] Connected
[Graph Stream] WebSocket opened
[Graph Stream] Received event: graph.screen.discovered
[Graph Stream] Adding new node: {screen_hash}
[Graph Stream] WebSocket closed (code: 1005)
\`\`\`

### Network Requests
- **Total Requests**: {request_count}
- **Failed Requests**: {failure_count}
- **Key Endpoints**:
  - GET /health → 200
  - POST /run → 200
  - GET /artifacts/* → 200 (x{artifact_count})

### Visual Proof
![Test Results](.cursor/test-artifacts/test-default-run-{timestamp}.png)

### Issues Found
{List any errors, warnings, or unexpected behavior}

### Recommendations
{Any suggested fixes or improvements}
```

---

## Error Handling

### If Backend Not Running
```markdown
❌ Test Failed: Backend not accessible

**Error**: Connection refused to http://localhost:{BACKEND_PORT}

**Fix**:
\`\`\`bash
cd backend
encore run
\`\`\`

**Port Detection**: {show detected ports}
```

### If Frontend Not Running
```markdown
❌ Test Failed: Frontend not accessible

**Error**: Connection refused to http://localhost:{FRONTEND_PORT}

**Fix**:
\`\`\`bash
cd frontend
bun run dev
\`\`\`

**Port Detection**: {show detected ports}
```

### If Button Not Found
```markdown
❌ Test Failed: "Detect My First Drift" button not found

**Snapshot**: {show accessibility tree}

**Possible Causes**:
1. Page not loaded (Svelte hydration pending)
2. Button CSS hidden
3. Frontend version mismatch

**Fix**: Wait longer or check frontend code
```

### If No Screenshots
```markdown
⚠️ Warning: No screenshots discovered

**Run Status**: {check if completed}
**Appium Status**: {check if running}
**Console Errors**: {show any errors}

**Possible Causes**:
1. Appium not running (device connection failed)
2. Run still in progress (wait longer)
3. Perceive action failed (check backend logs)

**Fix**: 
\`\`\`bash
# Check Appium
lsof -ti:4723

# Check backend logs for device errors
encore logs | grep -i device
\`\`\`
```

### If WebSocket Failed
```markdown
❌ Test Failed: WebSocket connection failed

**Console Logs**: 
\`\`\`
[Graph Stream] Connection failed
[Graph Stream] WebSocket closed (code: 1006)
\`\`\`

**Possible Causes**:
1. Backend not running
2. Run doesn't exist
3. Graph stream endpoint not available

**Fix**: Check backend logs and verify run ID exists
```

---

## Success Criteria

Test passes if ALL conditions met:

- [x] Backend and frontend running
- [x] Page loads without errors
- [x] "Detect My First Drift" button clickable
- [x] Run creation succeeds (POST /run → 200)
- [x] Navigation to /run/{id} succeeds
- [x] WebSocket stream connects
- [x] At least 1 screenshot discovered
- [x] At least 1 graph node rendered
- [x] No console errors (Vite warnings OK)
- [x] All artifact requests succeed (200)
- [x] Screenshot saved to test-artifacts/

---

## Troubleshooting Guide

### Port Issues
**Symptom**: Can't connect to localhost:{PORT}

**Check**:
```bash
# List all dev ports
lsof -ti:4000,4001,4002,5173,5174,5175

# Run port coordinator
bun backend/scripts/port-coordinator.mjs

# Check current directory
basename $(git rev-parse --show-toplevel)
```

### WebSocket Issues
**Symptom**: Console shows "Connection failed"

**Check**:
```bash
# Test WebSocket endpoint manually
wscat -c ws://localhost:4000/graph/run/01XXXXX/stream

# Check if stream endpoint registered
curl -s http://localhost:4000/__encore/health
```

### Screenshot Issues
**Symptom**: 0 screenshots discovered

**Check**:
1. Wait longer (agent flow needs 25-35s)
2. Check Appium: `lsof -ti:4723`
3. Check device connection: `adb devices`
4. Check backend logs: `encore logs | grep -i screenshot`
5. Check artifacts bucket: `encore db shell` → `SELECT * FROM artifacts LIMIT 10;`

---

## Command Execution

When this command is invoked, the AI should:

1. **Parse this file** to understand the test flow
2. **Detect ports** using port coordinator
3. **Verify services** running
4. **Invoke cursor-chrome-window-mastery skill** for browser automation
5. **Execute each step** sequentially
6. **Collect results** from browser interactions
7. **Generate report** in markdown format
8. **Save artifacts** to `.cursor/test-artifacts/`

**Estimated Duration**: 40-50 seconds total
- Port detection: 1s
- Service verification: 2s
- Browser launch: 3s
- Navigation: 2s
- Button click: 1s
- Agent flow: 30s
- Screenshot count: 2s
- Report generation: 1s

---

## Notes

- Uses **Google Chrome mode** for clean state
- Saves artifacts to `.cursor/test-artifacts/` (gitignored)
- Reports are markdown-formatted for easy reading
- Screenshots are timestamped to avoid conflicts
- Can be run repeatedly without state pollution

SKILL_INVOCATION

